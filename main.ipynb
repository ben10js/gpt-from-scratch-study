{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f154b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tk.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74652b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import OktTokenizer, create_okt_dataloader\n",
    "\n",
    "with open(\"verdict3.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tk = OktTokenizer(vocab_size=20000)\n",
    "pre_text = tk.preprocess(text)\n",
    "tokens = tk.tokenize(pre_text)\n",
    "tk.build_vocab(tokens)\n",
    "encoded = tk.encode(tokens, tk.vocab)\n",
    "\n",
    "\n",
    "# \ub370\uc774\ud130\ub85c\ub354 \uc0dd\uc131 \ubc0f \uac80\uc99d\n",
    "okt_dataloader = create_okt_dataloader(encoded, batch_size=4)\n",
    "for batch in okt_dataloader:\n",
    "    print(tk.decode(batch[0][0].numpy()))  # \uccab \uc0d8\ud50c \ud14d\uc2a4\ud2b8 \ubcf5\uc6d0\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c738e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ub0b4\uac00 \uc9c0\uc815\ud55c vocab_size:\", tk.vocab_size)\n",
    "print(\"\uc2e4\uc81c \ucf54\ud37c\uc2a4 \ub0b4 unique \ud1a0\ud070 \uc218:\", len(tk.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ub9cc\uc57d OktTokenizer \ud074\ub798\uc2a4\ucc98\ub7fc vocab\uc774 \ub0b4\ubd80\uc5d0 \uc788\uc744 \ub54c:\n",
    "print(\"Total vocab size:\", len(tk.vocab))\n",
    "print(\"Vocab \uc608\uc2dc 30\uac1c:\", list(tk.vocab.keys())[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"torch\",           # PyTorch - \ub525\ub7ec\ub2dd \ud504\ub808\uc784\uc6cc\ud06c\n",
    "        \"numpy\",           # \uc218\uce58 \uc5f0\uc0b0\n",
    "        \"matplotlib\"       # \uc2dc\uac01\ud654 (\uc120\ud0dd\uc0ac\ud56d)\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "print(torch.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567eba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) \ud1a0\ud06c\ub098\uc774\uc800 \uac1d\uccb4 \uc0dd\uc131(\ucf54\ud37c\uc2a4 vocab \uba3c\uc800 \ube4c\ub4dc\ud574\uc57c \uc0ac\uc6a9 \uac00\ub2a5!)\n",
    "tokenizer = OktTokenizer(vocab_size=20000)\n",
    "\n",
    "# 2) verdict3.txt \uc804\uccb4 \uc77d\uace0 \ud1a0\ud06c\ub098\uc774\uc800\uc5d0 vocab \uad6c\ucd95\n",
    "with open(\"verdict3.txt\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "pre_text = tokenizer.preprocess(raw_text)\n",
    "tokens = tokenizer.tokenize(pre_text)\n",
    "tokenizer.build_vocab(tokens)\n",
    "\n",
    "# 3) \ub2e8\uc77c \ud14c\uc2a4\ud2b8 \ubb38\uc7a5 \uc900\ube44\n",
    "start_context = \"\ubcf5\uc7a1\ud55c, \uad6c\ub450\uc810\uc774 '\uc5ec\uae30' \uc788\uc2b5\ub2c8\ub2e4! (\uc778\uc2dd\uc744 \uc798 \ud560\uae4c~) \ubb38\uc7a5\uc758 \ub05d\ub3c4 \uc798 \uc778\uc2dd\ud560\uae4c\uc694??\"\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    # \ubc18\ub4dc\uc2dc verdict3.txt \uae30\ubc18\uc73c\ub85c vocab\uc774 \ube4c\ub4dc\ub41c \uc0c1\ud0dc\uc784\n",
    "    pre = tokenizer.preprocess(text)\n",
    "    toks = tokenizer.tokenize(pre)\n",
    "    encoded = tokenizer.encode(toks, tokenizer.vocab)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # (1, sequence_length)\n",
    "    return encoded_tensor\n",
    "\n",
    "# 4) \ud14c\uc2a4\ud2b8 \uc2e4\ud589\n",
    "token_ids = text_to_token_ids(start_context, tokenizer)\n",
    "print(\"Test context shape:\", token_ids.shape)\n",
    "print(\"\ud1a0\ud06c\ub098 ID \ub9ac\uc2a4\ud2b8:\", token_ids)\n",
    "print(\"\ub514\ucf54\ub4dc \uacb0\uacfc:\", tokenizer.decode(token_ids.squeeze().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tk):\n",
    "    token_ids_list = token_ids.squeeze(0).tolist()  # Remove batch dimension and convert to list\n",
    "    text_list = tk.decode(token_ids_list)           # -> \ub9ac\uc2a4\ud2b8 \ubc18\ud658\n",
    "    text_str = \" \".join(text_list)                  # \uacf5\ubc31 join \ud574\uc11c \ubb38\uc790\uc5f4\ub85c\n",
    "    return text_str\n",
    "\n",
    "print(token_ids_to_text(token_ids, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d32500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import OktTokenizer, create_okt_dataloader\n",
    "\n",
    "tk = OktTokenizer()\n",
    "tokens = tk.tokenize(tk.preprocess(raw_text))  # \ubc18\ub4dc\uc2dc \ud1a0\ud070 \ub2e8\uc704 \ub9ac\uc2a4\ud2b8\ub85c!\n",
    "tk.build_vocab(tokens)    # vocab \uc0ac\uc804 \uad6c\ucd95\n",
    "\n",
    "ids = tk.encode(tokens)   # \ub9ac\uc2a4\ud2b8(int)\ub85c \ubcc0\ud658\n",
    "\n",
    "# train/val split\n",
    "split_idx = int(0.9 * len(ids))\n",
    "train_ids = ids[:split_idx]\n",
    "val_ids = ids[split_idx:]\n",
    "\n",
    "train_loader = create_okt_dataloader(train_ids, batch_size=2, max_length=256, stride=256, drop_last=True, shuffle=True)\n",
    "val_loader = create_okt_dataloader(val_ids, batch_size=2, max_length=256, stride=256, drop_last=False, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fefced",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": len(tk.vocab),\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)  # \uc9c1\uc811 \uad6c\ud604 \ub610\ub294 ch.4\uc5d0\uc11c \uac00\uc838\uc624\uae30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33214d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tk.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader batch \uc218:\", len(train_loader))\n",
    "print(\"Val loader batch \uc218:\", len(val_loader))\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Train batch {i}: shape {inputs.shape}, {targets.shape}\")\n",
    "    if i > 4: break  # \ucc98\uc74c \uba87 \uac1c\ub9cc \ucd9c\ub825\ud574\ub3c4 \ucda9\ubd84\n",
    "for i, (inputs, targets) in enumerate(val_loader):\n",
    "    print(f\"Val batch {i}: shape {inputs.shape}, {targets.shape}\")\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    inputs, targets = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(inputs)\n",
    "    # logits: (batch, seq, vocab), targets: (batch, seq)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        targets.view(-1)\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2acfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss, batches_counted = 0., 0\n",
    "\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        inputs = input_batch.to(device)\n",
    "        targets = target_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = calc_loss_batch(inputs, targets, model, device)\n",
    "            total_loss += loss.item()\n",
    "            batches_counted += 1\n",
    "        if num_batches is not None and batches_counted >= num_batches:\n",
    "            break\n",
    "\n",
    "    avg_loss = total_loss / batches_counted if batches_counted > 0 else float(\"inf\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_text_simple\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # \ud2b8\ub799\uc6a9 \ub9ac\uc2a4\ud2b8\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} \uc2dc\uc791\")\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # \ud3c9\uac00 \uc8fc\uae30\uc77c \ub54c\ub9cc loss/\uc131\ub2a5 log\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        \n",
    "        # \uc5d0\ud3ed \ub05d\ub0a0 \ub54c\ub9c8\ub2e4 \uc0d8\ud50c \uc0dd\uc131\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f904bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# \uc2dc\uac04 \uce21\uc815 (\uc120\ud0dd\uc0ac\ud56d)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)  # (\ub610\ub294 set_seed \ud568\uc218\ub85c \ub354 \uc815\ubc00\ud558\uac8c)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=100, eval_iter=5,\n",
    "    start_context=\"\uc9c1\uc811\uacbd\ud5d8\uc740 \uc911\uc694\ud574\uc9c0\uace0 \uc788\ub2e4. \ub098\ub294 \ub108\ub97c\", tokenizer=tk\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907734e4",
   "metadata": {},
   "source": [
    "# 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8769b7c",
   "metadata": {},
   "source": [
    "# 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b973931",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1039c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}