{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e62428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"torch\",           # PyTorch - \ub525\ub7ec\ub2dd \ud504\ub808\uc784\uc6cc\ud06c\n",
    "        \"numpy\",           # \uc218\uce58 \uc5f0\uc0b0\n",
    "        \"matplotlib\"       # \uc2dc\uac01\ud654 (\uc120\ud0dd\uc0ac\ud56d)\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "print(torch.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_META = {\n",
    "    \"vocab_size\": 1203,         # \uadf8\ub0e5 \ubc14\ub85c \uc0ac\uc6a9 (model, tokenizer, config \ubaa8\ub450 \ud1b5\uc77c!)\n",
    "    \"context_length\": 256,      # \ubb38\uc7a5/\ubb38\ub2e8/\ucc57\ubd07 \uc0c1\ud669 \uad8c\uc7a5\uac12 (\uc9e7\uc740 \ud14d\uc2a4\ud2b8\ub098 \uba54\ubaa8\ub9ac \uc791\uc744 \ub54c\ub294 128\ub85c\ub3c4 OK)\n",
    "    \"emb_dim\": 256,             # \uc784\ubca0\ub529 \ucc28\uc6d0 (\uc791\uc740 \ubaa8\ub378 \uc2e4\ud5d8\uc5d0 \uad8c\uc7a5)\n",
    "    \"n_heads\": 4,               # attention head (emb_dim \uae30\uc900 4~8\uc774 \uc77c\ubc18\uc801)\n",
    "    \"n_layers\": 4,              # transformer \ube14\ub7ed \uc218 (\uc791\uc740 \ubaa8\ub378 \uc2e4\ud5d8\uc5d0 \uad8c\uc7a5)\n",
    "    \"drop_rate\": 0.1,           # \uc18c\uaddc\ubaa8\uc5d4 0.1, overfitting \ud06c\uba74 0.2 \ucd94\ucc9c\n",
    "    \"qkv_bias\": False           # False\ub85c \ub450\ub294 \uac8c \uc77c\ubc18\uc801 (\uae30\ubcf8 \uc138\ud305)\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_META)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomTokenizer\n",
    "\n",
    "# 1) \ucd08\uae30\ud654\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "# 2) verdict4.txt \ubd88\ub7ec\uc624\uae30\n",
    "with open(\"verdict4.txt\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# 3) \uc804\ucc98\ub9ac (\uae30\uc874 <EOS>/<PARA_END>\uac00 \uc788\ub294 \uacbd\uc6b0)\n",
    "preprocessed = tokenizer.preprocess(txt)\n",
    "\n",
    "# 4) \uc11c\ube0c\uc6cc\ub4dc \ubcd1\ud569 \uaddc\uce59 \ud559\uc2b5 (\uc18c\ud615 \ud14c\uc2a4\ud2b8\uba74 vocab_size=300)\n",
    "tokenizer.learn_bpe(preprocessed, vocab_size=300)\n",
    "print(\"\ud559\uc2b5\ub41c \ubcd1\ud569 \uaddc\uce59 \uc218:\", len(tokenizer.merges))\n",
    "\n",
    "# 5) \ud1a0\ud070\ud654 \ud14c\uc2a4\ud2b8\n",
    "sample = \"\uc9c1\uc811\uacbd\ud5d8\uc740 \uc911\uc694\ud574\uc9c0\uace0 \uc788\ub2e4. \ub098\ub294 \ub108\ub97c \uc0ac\ub791\ud55c\ub2e4. <EOS> <PARA_END>\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"\\n\uc785\ub825 \ubb38\uc7a5:\", sample)\n",
    "print(\"\u2192 \ud1a0\ud070 IDs:\", encoded[:40])\n",
    "print(\"\u2192 \ubcf5\uc6d0 \uacb0\uacfc:\", decoded[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"\ubcf5\uc7a1\ud55c, \uad6c\ub450\uc810\uc774 '\uc5ec\uae30' \uc788\uc2b5\ub2c8\ub2e4! (\uc778\uc2dd\uc744 \uc798 \ud560\uae4c~) \ubb38\uc7a5\uc758 \ub05d\ub3c4 \uc798 \uc778\uc2dd\ud560\uae4c\uc694??\"\n",
    "\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "token_ids = text_to_token_ids(start_context, tokenizer)\n",
    "print(token_ids.shape)  # Should be (1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from utils import MetaTokenizer\n",
    "tokenizer = MetaTokenizer('mymeta_tokenizer4.model')\n",
    "start_context = \"\ubcf5\uc7a1\ud55c, \uad6c\ub450\uc810\uc774 '\uc5ec\uae30' \uc788\uc2b5\ub2c8\ub2e4! (\uc778\uc2dd\uc744 \uc798 \ud560\uae4c~) \ubb38\uc7a5\uc758 \ub05d\ub3c4 \uc798 \uc778\uc2dd\ud560\uae4c\uc694??\"\n",
    "'''\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "token_ids = text_to_token_ids(start_context, tokenizer)\n",
    "print(token_ids.shape)  # Should be (1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7684f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    token_ids_list = token_ids.squeeze(0).tolist()  # Remove batch dimension and convert to list\n",
    "    text = tokenizer.decode(token_ids_list)\n",
    "    return text\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c178d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_text_simple\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=token_ids,\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_META['context_length'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min:\", min(token_ids), \"max:\", max(token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"mymeta_tokenizer4.model\")\n",
    "print(sp.get_piece_size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916f386",
   "metadata": {},
   "source": [
    "# 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))  # 0\ubc88 GPU \uc774\ub984 \ucd9c\ub825\n",
    "    print(torch.cuda.device_count())      # GPU \uac1c\uc218 \ucd9c\ub825\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65305a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import MetaTokenizer, create_dataloader_v2\n",
    "\n",
    "tokenizer = MetaTokenizer(\"mymeta_tokenizer4.model\")\n",
    "file_path = \"verdict4.txt\"\n",
    "with open(\"verdict4.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "ids = tokenizer.encode(raw_text)\n",
    "\n",
    "# \ud6c8\ub828/\uac80\uc99d\uc14b \ub098\ub204\uae30\n",
    "split_idx = int(0.9 * len(ids))\n",
    "ids_train = ids[:split_idx]\n",
    "ids_val = ids[split_idx:]\n",
    "\n",
    "train_loader = create_dataloader_v2(\n",
    "    ids_train, batch_size=32, max_length=256, stride=128, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader = create_dataloader_v2(\n",
    "    ids_val, batch_size=32, max_length=256, stride=128, shuffle=False, drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader batch \uc218:\", len(train_loader))\n",
    "print(\"Val loader batch \uc218:\", len(val_loader))\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Train batch {i}: shape {inputs.shape}, {targets.shape}\")\n",
    "    if i > 4: break  # \ucc98\uc74c \uba87 \uac1c\ub9cc \ucd9c\ub825\ud574\ub3c4 \ucda9\ubd84\n",
    "for i, (inputs, targets) in enumerate(val_loader):\n",
    "    print(f\"Val batch {i}: shape {inputs.shape}, {targets.shape}\")\n",
    "    if i > 4: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af603e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    inputs, targets = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(inputs)\n",
    "    # logits: (batch, seq, vocab), targets: (batch, seq)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        targets.view(-1)\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss, batches_counted = 0., 0\n",
    "\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        inputs = input_batch.to(device)\n",
    "        targets = target_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = calc_loss_batch(inputs, targets, model, device)\n",
    "            total_loss += loss.item()\n",
    "            batches_counted += 1\n",
    "        if num_batches is not None and batches_counted >= num_batches:\n",
    "            break\n",
    "\n",
    "    avg_loss = total_loss / batches_counted if batches_counted > 0 else float(\"inf\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # \ud2b8\ub799\uc6a9 \ub9ac\uc2a4\ud2b8\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # \ud3c9\uac00 \uc8fc\uae30\uc77c \ub54c\ub9cc loss/\uc131\ub2a5 log\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        \n",
    "        # \uc5d0\ud3ed \ub05d\ub0a0 \ub54c\ub9c8\ub2e4 \uc0d8\ud50c \uc0dd\uc131\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c009ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# \uc2dc\uac04 \uce21\uc815 (\uc120\ud0dd\uc0ac\ud56d)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)  # (\ub610\ub294 set_seed \ud568\uc218\ub85c \ub354 \uc815\ubc00\ud558\uac8c)\n",
    "model = GPTModel(GPT_CONFIG_META)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"\uc9c1\uc811\uacbd\ud5d8\uc740 \uc911\uc694\ud574\uc9c0\uace0 \uc788\ub2e4. \ub098\ub294 \ub108\ub97c\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72dc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymeta_tokenizer2.model')\n",
    "print(sp.encode('\uc5ec\uae30\uc5d0 \ud638\ud638 \uc785\ub825'))\n",
    "print(sp.decode(sp.encode('\uc5ec\uae30\uc5d0 \ud638\ud638 \uc785\ub825')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}